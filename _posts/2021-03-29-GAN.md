# GAN(Generative Adversarial Networks)

### Main idea
- 위조 지폐범(Generator)와 경찰(Discriminator)의 대결
- G는 최대한 진짜 같은 모조품을 만들며, D는 진품과 모조품을 구별한다. 
- 경쟁적 학습을 통해 결국 완전한 모조품을 만드는 것이 목표(위조 지폐범의 승리)  

![image](https://user-images.githubusercontent.com/76815825/112716874-48582000-8f2c-11eb-994f-4a2a2319f10d.png)

GAN은 실존하지는 않지만 있을법한 이미지를 만들어내기 위한 모델로 게임이론의 minimax 알고리즘으로부터 아이디어를 따왔다고 한다. 위조지폐범(G)은 경찰(D)에게 걸리면 더 진짜 같은 위조지폐를 만들려하고 경찰은 위조지폐범을 잡기 위한 기술을 또 만들어내며 서로 발전하게 되는데 이러한 모습 때문에 적대적(Adversarial)이라고 표현한다. 현실과는 달리 이 모델에서는 위조지폐범(G)의 승리를 목표로 하며 결국 경찰(D)이 진짜와 가짜를 분간할 수 없는 지폐(생성 이미지)를 만들어내게 된다.
  
  
### Loss function
GAN의 손실함수는 다음과 같다.

![image](https://user-images.githubusercontent.com/76815825/112717394-b4885300-8f2f-11eb-8359-3a58892ef874.png)

  
식이 다소 복잡하니 하나하나 뜯어서 해석해보자. 우변은 다음과 같이 해석할 수 있다.  
Ex-Pdata(x)[logD(x)]: 실제 이미지의 데이터 분포(Pdata(x))에서 하나를 샘플링(x)하여 판별자에 넣었을 때 나온 값의 로그값의 기댓값  
Ez-Pz(z)[log(1-D(G(z)))]: noise 벡터 분포(Pz(z))에서 하나를 샘플링(z)하여 만든 생성이미지(G(z))를 판별자에 넣었을 때 나온 값의 로그값의 기댓값  

판별자D 입장에서는 x는 진짜(1)로 판별하고 z는 가짜(0)로 판별하여야 하므로 우변을 최대화하는 방향으로 학습하게 된다.  
생성자G 입장에서는 D가 G(z)를 1로 판별하게 만들고싶으므로 우변을 최소화하는 방향으로 학습하게 된다.  
  
  
### Optimality
![image](https://user-images.githubusercontent.com/76815825/112717896-db945400-8f32-11eb-8f9d-0c34105e763f.png)

위 그림은 생성자가 학습을 통해 원본데이터로 수렴하면서 목표를 달성하는 모습을 보여준다. 생성 모델의 분포인 초록선은 원본 데이터의 분포인 검정선과 같아지도록 점점 변하며 파란선은 판별 모델의 분포를 의미한다. 학습이 전혀 안 된 상태(a)에서는 불안정한 모습을 보이다가 점차 원본 데이터는 1에 가깝게 생성 데이터는 0에 가깝에 분류하기 시작했고 최종적으로는 1/2로 수렴하게 된다. 판별자에게 1/2이라는 값은 두가지로 분류하는데 있어 가장 애매하다는 의미라는 것을 쉽게 받아들일 수 있을 것이다.  

GAN의 Global Optimum point는 G가 고정되어 있을 때 D*G(x) = Pdata(x)/Pdata(x)+Pg(x)이며 Pg=Pdata인데 이 두 조건이 만족할 때 D(G(z))는 1/2로 수렴한다. 이 식을 도출하는 자세한 증명과정의 나동빈님의 유튜브 강의에서 볼 수 있다.  
<https://www.youtube.com/watch?v=AVvlDmhHgC4&t=1777s>


# VAE

### VAE vs AE
VAE의 구조를 보면 Autoencoder와 굉장히 유사하여 둘의 차이가 극명함에도 불구하고 처음에는 이를 이해하기가 쉽지 않다. 간단히 말하자면 이 둘은 애초에 목적이 다르다. AE는 차원축소가 목적이다. input으로부터 z벡터를 얻어내고자 하므로 인코더가 핵심이다. PCA와 같은 역할을 한다고 보면 된다. 반면, VAE는 GAN과 같은 생성모델이다. 이미지를 생성하기 위한 z값이 필요한데 이를 추정하기 위해 인코더를 붙이는 AE와 같은 구조를 갖게되었을 뿐이라고 생각하면 된다. 

- AE: 차원축소가 목적, Encoder가 핵심
- VAE: 이미지 생성이 목적, Decoder가 핵심

### VAE의 구조
![image](https://user-images.githubusercontent.com/76815825/112861671-fa8c1500-90ef-11eb-95bd-0e82275594d8.png)
**1. Encoder는 가우시안 분포를 따른다고 가정하여 평균과 표준편차로 이루어진 정규분포에서 zi를 샘플링한다.**

![image](https://user-images.githubusercontent.com/76815825/112861936-3b842980-90f0-11eb-90df-2469f293f18f.png)
**2. 베르누이 분포를 따른다고 가정한 Decoder를 통해 이미지를 복원한다. 이 때, 문제점이 한가지 있는데 학습 시 역전파를 위한 미분을 할 방법이 없다는 것이다. 그리하여 중간에 한 가지의 절차를 더해준다.**

![image](https://user-images.githubusercontent.com/76815825/112862249-87cf6980-90f0-11eb-8c53-0cc049860723.png)
**3. 표준정규로부터 아주 작은 값인 epsilon을 샘플링 하여 표준편차를 곱하고 평균을 더한 값을 zi라 한다(𝜇+𝜀∙𝜎)  
     이를 Reparameterization Trick이라고 부른다.**
     

### Loss function
![image](https://user-images.githubusercontent.com/76815825/112863861-3fb14680-90f2-11eb-943b-558d7a0f1335.png)
VAE의 loss function은 두 가지로 구성된다.
1) **Reconstruction Error**: output과 input의 이미지의 차이를 최소화시킨다 -> cross entropy
2) **Regularization Error**: Encoder를 통과한 값의 분포와 정규분포의 차이를 최소화시킨다 -> KL divergence
![image](https://user-images.githubusercontent.com/76815825/112864476-cf56f500-90f2-11eb-9acd-36f040fdd667.png)


# DCGAN
![image](https://user-images.githubusercontent.com/76815825/112864845-2fe63200-90f3-11eb-8c2a-cc6c4423afd5.png)

### 특징
- CNN의 기법을 활용 -> fully connected 구조 대신 convolution, pooling, padding 활용
- 모든 layer에 batch-normalization 적용
- Generator에서는 모든 활성화함수를 Relu를 쓰되, 마지막 출력층에서만 Tanh
- Discriminator에서는 모든 활성화 함수로 LeakyRelu 사용 

### 성과
- **Walking in the latent space**  
![image](https://user-images.githubusercontent.com/76815825/112865502-ce729300-90f3-11eb-9a22-053b75ee5ba2.png)
GAN에 대한 문제점 중 하나가 이미지를 생성하는 것이 아니라 수많은 데이터로 인해 기억을 해서 변형시키는 것 아니냐라는 의문에 해답을 내놓지 못했다는 것인데 DCGAN이 이 부분을 해소시켜주었다. 위 그림의 마지막 줄을 보면 input인 z값에 미세하게 변화를 주자 서서히 창문이 생기는 모습을 볼 수 있는데 이로 인해 GAN이 이미지를 기억하는 것이 아니라 생성하는 것이다라는 증명을 보일 수 있었다.

- **Applying arithmetic in the input space**
![image](https://user-images.githubusercontent.com/76815825/112866869-3f667a80-90f5-11eb-98e5-eef3fd2d7870.png)ㅣ
GAN에 대한 문제점 중 하나가 이미지를 생성하는 것이 아니라 수많은 데이터로 인해 기억을 해서 변형시키는 것 아니냐라는 의문에 해답을 내놓지 못했다는 것인데 DCGAN이 이 부분을 해소시켜주었다. 위 그림의 마지막 줄을 보면 input인 z값에 미세하게 변화를 주자 서서히 창문이 생기는 모습을 볼 수 있는데 이로 인해 GAN이 이미지를 기억하는 것이 아니라 생성하는 것이다라는 증명을 보일 수 
