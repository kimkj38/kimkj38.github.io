# GAN

## Main idea
- 위조 지폐범(Generator)와 경찰(Discriminator)의 대결
- G는 최대한 진짜 같은 모조품을 만들며, D는 진품과 모조품을 구별한다. 
- 경쟁적 학습을 통해 결국 완전한 모조품을 만드는 것이 목표(위조 지폐범의 승리)  

![image](https://user-images.githubusercontent.com/76815825/112716874-48582000-8f2c-11eb-994f-4a2a2319f10d.png)

GAN은 실존하지는 않지만 있을법한 이미지를 만들어내기 위한 모델이다. 진짜와 유사한 이미지를 형성하기 위해 진짜와 가짜를 구별하는 Discriminator를 통해 지속적인 학습이 가능하며, 최종적으로는 Discriminator가
구별하지 못할 정도로 진짜에 가까운 이미지를 생성한다.  
  
  
## Data of image
사람이 이미지를 구분하는 것은 직관적이다. 하지만 기계는 무슨 근거로 이미지가 유사성을 판단할까?
![image](https://user-images.githubusercontent.com/76815825/112717057-71c57b80-8f2d-11eb-87b5-a919ee1d3dfb.png)

사실 사람도 학습에 의해 직관적으로 인식하게 되었을 뿐이지 세상에 갓 태어난 아기가 바라보는 세상의 이미지들은 학습 전의 기계와 비슷할 것이다. 아기들이 경험을 통해 사람과 강아지의 눈, 코, 입 등을 구별하듯이 기계도
이미지 내의 데이터로 학습을 하게 된다. 이 때 같은 카테고리의 이미지들은 평균적인 통계치가 존재하여 각 특징별로 위 그림과 같은 확률분포를 보인다고 생각해볼 수 있다.  
  
  
## Loss function
그렇다면 이제 우리는 손실함수를 통해 생성이미지가 실제 이미지의 확률분포와 최대한 유사해지도록 학습시켜야 한다. 손실함수는 다음과 같다.

![image](https://user-images.githubusercontent.com/76815825/112717394-b4885300-8f2f-11eb-8359-3a58892ef874.png)

식이 다소 복잡하게 보이니 하나하나 뜯어서 해석해보자.  

우변은 다음과 같이 해석할 수 있다.  
Ex-Pdata(x)[logD(x)]: 실제 이미지의 데이터 분포(Pdata(x))에서 하나를 샘플링(x)하여 판별자에 넣었을 때 나온 값의 로그값의 기댓값  
Ez-Pz(z)[log(1-D(G(z)))]: noise 벡터 분포(Pz(z))에서 하나를 샘플링(z)하여 만든 생성이미지(G(z))를 판별자에 넣었을 때 나온 값의 로그값의 기댓값  

판별자D 입장에서는 x는 진짜(1)로 판별하고 z는 가짜(0)로 판별하여야 하므로 우변을 최대화하는 방향으로 학습하게 된다.  
생성자G 입장에서는 D가 G(z)를 1로 판별하게 만들고싶으므로 우변을 최소화하는 방향으로 학습하게 된다.  
  
  
## Optimality
![image](https://user-images.githubusercontent.com/76815825/112717896-db945400-8f32-11eb-8f9d-0c34105e763f.png)

위의 그림과 같이 생성 모델과 판별 모델의 분포가 일치하는 방향으로 학습하게 되는데 이 때 D(G(z))는 1/2로 수렴한다.
필자는 이 값을 도출하는 과정이 논문의 식만으로는 이해가 가지 않아 나동빈님의 강의 영상을 참고하였다.  
![image](https://user-images.githubusercontent.com/76815825/112718025-be13ba00-8f33-11eb-8aee-d3d86fca2fd6.png)
![image](https://user-images.githubusercontent.com/76815825/112718016-ab998080-8f33-11eb-869d-220f2614de61.png)  

