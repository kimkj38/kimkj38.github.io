---
title:  "DCGAN구현"

categories:
  - 코딩
tags:
  - VAE
  - 코딩
  - pytorch
classes: wide
typora-copy-images-to: ..\images\2021-03-20

---

__references의 사이트를 따라 필사한 코드입니다.__  

```python
from __future__ import print_function
#%matplotlib inline
import argparse
import os
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML

print('Pytorch Version =', torch.__version__)


import random

manual_seed = 1004
print("Random Seed =", manual_seed)
random.seed(manual_seed)
torch_manual_seed = torch.manual_seed(manual_seed)
```

    Pytorch Version = 1.8.0+cu101
    Random Seed = 1004



```python
from torchvision import datasets
import torchvision.utils as vutils
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

transform = transforms.Compose([
    transforms.Resize((32, 32)),    # 이미지 크기 28 x 28 -> 32 x 32로 변경
    transforms.ToTensor(),          # Pytorch Tensor 형식으로 변경
    transforms.Normalize((0.5,), (0.5,)) # 이미지 스케일 변경[0, 1] -> [-1, 1]
])

# MNIST 데이터셋을 다운로드 받습니다.
datasets.MNIST.resources = [
            ('https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz', 'f68b3c2dcbeaaa9fbdd348bbdeb94873'),
            ('https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz', 'd53e105ee54ea40749a09fcbcd1e9432'),
            ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz', '9fb629c4189551a2d022fa330f9573f3'),
            ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz', 'ec29112dd5afa0611ce80d1b7f02629c'), 
        ]
dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)
print('총 데이터 갯수 =', len(dataset))
print('이미지 크기 [channel, y, x] =', dataset[0][0].size())    # 그레이스케일이기 때문에 channel은 1

# 이후 학습과정에서 매 epoch마다 데이터셋을 섞고 배치 단위로 불러오기 위해 DataLoader를 정의합니다.
dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)
training_imgs = next(iter(dataloader))[0]
print('미니배치 데이터 크기 [batch size, channel, y, x]=', training_imgs.size())
```

    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz



    HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))


    
    Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz



    HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))


    
    Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz



    HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))


    
    Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz



    HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))


    
    Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw
    Processing...


    /usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)
      return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)


    Done!
    총 데이터 갯수 = 60000
    이미지 크기 [channel, y, x] = torch.Size([1, 32, 32])


    /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      cpuset_checked))


    미니배치 데이터 크기 [batch size, channel, y, x]= torch.Size([64, 1, 32, 32])



```python
import numpy as np
import matplotlib.pyplot as plt

training_imgs = vutils.make_grid(training_imgs[:25], padding=2, nrow=5, normalize=True)
fig = plt.figure(figsize=(5, 5))
plt.title('MNIST Data')
plt.axis('off')
# Pytorch는 [Channel, Y, X] 형식으로 다루고
# matplotlib나 opencv의 경우 [Y, X, Channel의 형식]을 다루기 때문에
# np.transpose(tensor, (1, 2, 0))로 축의 순서를 바꿔줍니다.
plt.imshow(np.transpose(training_imgs, (1, 2, 0)))
plt.show()
```


    
![png](https://kimkj38.github.io/images/2021-03-20/DCGAN(MNIST)_2_0.png)
    



```python
import torch.nn as nn

Z_DIM = 128        # Generator의 입력으로 사용할 Z의 차원수
N_FEATURES = 64    # CNN의 feature개수
N_CHANNELS = 1     # 이미지의 채널개수 [GrayScale]

# Initialize BCELoss function
criterion = nn.BCELoss()

G = nn.Sequential(
    # input size = Z_DIM x 1 x 1
    nn.ConvTranspose2d(Z_DIM, N_FEATURES * 4, kernel_size=4, stride=1, padding=0, bias=False),
    nn.BatchNorm2d(N_FEATURES * 4),
    nn.ReLU(inplace=True),
    # feature size = (N_FEATURES * 4) x 4 x 4
    nn.ConvTranspose2d(N_FEATURES * 4, N_FEATURES * 2, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(N_FEATURES * 2),
    nn.ReLU(inplace=True),
    # feature size = (N_FEATURES * 2) x 8 x 8
    nn.ConvTranspose2d(N_FEATURES * 2, N_FEATURES, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(N_FEATURES),
    nn.ReLU(inplace=True),
    # feature size = N_FEATURES x 16 x 16
    nn.ConvTranspose2d(N_FEATURES, N_CHANNELS,  kernel_size=4, stride=2, padding=1, bias=True),
    nn.Tanh()
    # output size = N_CHANNELS x 32 x 32,  range = [-1, 1]
)

D = nn.Sequential(
    # input size = N_CHANNELS x 32 x 32,  range = [-1, 1]
    nn.Conv2d(N_CHANNELS, N_FEATURES, kernel_size=4, stride=2, padding=1, bias=True),
    nn.LeakyReLU(0.2, inplace=True),
    # feature size = N_FEATURES x 16 x 16
    nn.Conv2d(N_FEATURES, N_FEATURES * 2, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(N_FEATURES * 2),
    nn.LeakyReLU(0.2, inplace=True),
    # feature size = (N_FEATURES * 2) x 8 x 8
    nn.Conv2d(N_FEATURES * 2, N_FEATURES * 4, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(N_FEATURES * 4),
    nn.LeakyReLU(0.2, inplace=True),
    # feature size = (N_FEATURES * 4) x 4 x 4
    nn.Conv2d(N_FEATURES * 4, 1,  kernel_size=4, stride=1, padding=0, bias=True),
    nn.Sigmoid()
    # output size = 1 x 1 x 1,  range = [0, 1]
)

def weights_init(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
        nn.init.normal_(m.weight, mean=0.0, std=0.02)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.normal_(m.weight, mean=1.0, std=0.02)
        nn.init.zeros_(m.bias)

G.apply(weights_init)
D.apply(weights_init)
```




    Sequential(
      (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): LeakyReLU(negative_slope=0.2, inplace=True)
      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.2, inplace=True)
      (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1))
      (9): Sigmoid()
    )




```python
from tqdm import tqdm, tqdm_notebook    # progress bar

TOTAL_EPOCH = 10

G_losses = []
D_losses = []

fixed_noise = torch.rand(25, Z_DIM, 1, 1) * 2 - 1    # [-1, 1]
fixed_imgs = []
iteration = 0
for epoch in range(TOTAL_EPOCH):
    pbar = tqdm(dataloader)
    for real_data, _ in pbar:    # 레이블 정보는 필요 없기 때문에 _처리 했습니다.
        batch_size = real_data.size(0)
        real_label = torch.ones(batch_size)    # 1
        fake_label = torch.zeros(batch_size)    # 0
        
         # Setup Adam optimizers for both G and D
        lr = 0.0002
        beta1 = 0.5
        optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))
        optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))

        # Update D: maximize log(D(x)) + log(1 - D(G(z)))
        

        # Real Data
        D.zero_grad()    # 그레디언트 정보 초기화
        pred_real_label = D(real_data)
        pred_real_label = pred_real_label.squeeze()    # [batch_size, 1, 1, 1] -> [batch_size]

        D_real_loss = criterion(pred_real_label, real_label)
        D_real_loss.backward()

        # Fake Data
        z_noise = torch.rand(batch_size, Z_DIM, 1, 1, device=device) * 2 - 1    # [-1, 1]
        fake_img = G(z_noise)
        pred_fake_label = D(fake_img)
        pred_fake_label = pred_fake_label.squeeze()

        D_fake_loss = criterion(pred_fake_label, fake_label)
        D_fake_loss.backward()

        D_loss = D_real_loss + D_fake_loss
        optimizer_D.step()

        # Update G: maximize log(D(G(z)))
        G.zero_grad()
        z_noise = torch.rand(batch_size, Z_DIM, 1, 1, device=device) * 2 - 1    # [-1, 1]
        fake_img = G(z_noise)
        pred_fake_label = D(fake_img)
        pred_fake_label = pred_fake_label.squeeze()

        G_loss = criterion(pred_fake_label, real_label)
        G_loss.backward()

        optimizer_G.step()

        pbar.set_description('epoch={:d}/{:d}, D_loss={:.4f}, G_loss{:.4f}'.format(
            epoch + 1, TOTAL_EPOCH, D_loss.item(), G_loss.item()
        ))
        
        D_losses.append(D_loss.item())
        G_losses.append(G_loss.item())

        if iteration % 200 == 0:    # 200번 학습마다 학습이미지 저장
            fixed_img = G(fixed_noise).detach().cpu()
            fixed_img = vutils.make_grid(fixed_img, padding=2, nrow=5, normalize=True)
            fixed_imgs.append(fixed_img)

        iteration += 1
    # end for dataloader
# end for epoch
fixed_img = G(fixed_noise).detach().cpu()
fixed_img = vutils.make_grid(fixed_img, padding=2, nrow=5, normalize=True)
fixed_imgs.append(fixed_img)
```
출력 내용이 길어 일부 삭제 하였습니다. 
    
    
    epoch=10/10, D_loss=0.9201, G_loss3.9442:  98%|█████████▊| 916/938 [17:41<00:25,  1.18s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.9201, G_loss3.9442:  98%|█████████▊| 917/938 [17:41<00:24,  1.17s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1598, G_loss6.5963:  98%|█████████▊| 917/938 [17:42<00:24,  1.17s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1598, G_loss6.5963:  98%|█████████▊| 918/938 [17:42<00:23,  1.18s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.5452, G_loss3.8904:  98%|█████████▊| 918/938 [17:43<00:23,  1.18s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.5452, G_loss3.8904:  98%|█████████▊| 919/938 [17:43<00:22,  1.17s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.3147, G_loss6.5658:  98%|█████████▊| 919/938 [17:44<00:22,  1.17s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.3147, G_loss6.5658:  98%|█████████▊| 920/938 [17:44<00:20,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=1.1368, G_loss4.2709:  98%|█████████▊| 920/938 [17:45<00:20,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=1.1368, G_loss4.2709:  98%|█████████▊| 921/938 [17:45<00:19,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1735, G_loss6.5649:  98%|█████████▊| 921/938 [17:46<00:19,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1735, G_loss6.5649:  98%|█████████▊| 922/938 [17:46<00:18,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.8030, G_loss4.1977:  98%|█████████▊| 922/938 [17:48<00:18,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.8030, G_loss4.1977:  98%|█████████▊| 923/938 [17:48<00:17,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2595, G_loss7.0210:  98%|█████████▊| 923/938 [17:49<00:17,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2595, G_loss7.0210:  99%|█████████▊| 924/938 [17:49<00:16,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.6192, G_loss3.9618:  99%|█████████▊| 924/938 [17:50<00:16,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.6192, G_loss3.9618:  99%|█████████▊| 925/938 [17:50<00:15,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2353, G_loss6.9058:  99%|█████████▊| 925/938 [17:51<00:15,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2353, G_loss6.9058:  99%|█████████▊| 926/938 [17:51<00:13,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=1.0000, G_loss3.8371:  99%|█████████▊| 926/938 [17:52<00:13,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=1.0000, G_loss3.8371:  99%|█████████▉| 927/938 [17:52<00:12,  1.17s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2077, G_loss7.2413:  99%|█████████▉| 927/938 [17:53<00:12,  1.17s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2077, G_loss7.2413:  99%|█████████▉| 928/938 [17:53<00:11,  1.17s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.4693, G_loss3.7618:  99%|█████████▉| 928/938 [17:55<00:11,  1.17s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.4693, G_loss3.7618:  99%|█████████▉| 929/938 [17:55<00:10,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2087, G_loss6.7963:  99%|█████████▉| 929/938 [17:56<00:10,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2087, G_loss6.7963:  99%|█████████▉| 930/938 [17:56<00:09,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2999, G_loss4.5251:  99%|█████████▉| 930/938 [17:57<00:09,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2999, G_loss4.5251:  99%|█████████▉| 931/938 [17:57<00:08,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1992, G_loss6.3717:  99%|█████████▉| 931/938 [17:58<00:08,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1992, G_loss6.3717:  99%|█████████▉| 932/938 [17:58<00:06,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2380, G_loss3.8587:  99%|█████████▉| 932/938 [17:59<00:06,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2380, G_loss3.8587:  99%|█████████▉| 933/938 [17:59<00:05,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2013, G_loss6.7042:  99%|█████████▉| 933/938 [18:00<00:05,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2013, G_loss6.7042: 100%|█████████▉| 934/938 [18:00<00:04,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1738, G_loss3.7601: 100%|█████████▉| 934/938 [18:01<00:04,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1738, G_loss3.7601: 100%|█████████▉| 935/938 [18:01<00:03,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2289, G_loss6.9193: 100%|█████████▉| 935/938 [18:03<00:03,  1.15s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2289, G_loss6.9193: 100%|█████████▉| 936/938 [18:03<00:02,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2268, G_loss3.9931: 100%|█████████▉| 936/938 [18:04<00:02,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.2268, G_loss3.9931: 100%|█████████▉| 937/938 [18:04<00:01,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1574, G_loss6.1835: 100%|█████████▉| 937/938 [18:04<00:01,  1.16s/it][A[A[A[A[A[A[A[A
    
    
    
    
    
    
    
    epoch=10/10, D_loss=0.1574, G_loss6.1835: 100%|██████████| 938/938 [18:04<00:00,  1.16s/it]



```


```python
fig = plt.figure(figsize=(10, 5))
ax1 = fig.add_subplot(1, 2, 1)
ax2 = fig.add_subplot(1, 2, 2)
ax1.axis('off')
ax2.axis('off')
ax1.title.set_text('Generated Images')
ax2.title.set_text('Training Images')

ax1.imshow(np.transpose(fixed_imgs[-1], (1, 2, 0)))
ax2.imshow(np.transpose(training_imgs, (1, 2, 0)))
plt.show()
```


    
![png](https://kimkj38.github.io/images/2021-03-20/DCGAN(MNIST)_7_0.png)
    



```python
N_POINT = 5
N_INTERPOLATION = 11

pointsA = torch.rand(N_POINT, Z_DIM, 1, 1) * 2 - 1    # [-1, 1]
pointsB = torch.rand(N_POINT, Z_DIM, 1, 1) * 2 - 1    # [-1, 1]

line = []
for pointA, pointB in zip(pointsA, pointsB):
    for alpha in np.linspace(start=0.0, stop=1.0, num=N_INTERPOLATION):
        z = alpha * pointA + (1.0 - alpha) * pointB
        line.append(z)

line = torch.stack(line)

  # to GPU
line_images = G(line)
interpolation = vutils.make_grid(line_images.detach().cpu(), nrow=N_INTERPOLATION, padding=2, normalize=True)

fig = plt.figure(figsize=(N_INTERPOLATION, N_POINT))
plt.axis('off')
plt.title('Interpolation')
plt.imshow(np.transpose(interpolation, (1, 2, 0)))
plt.show()
```


    
![png](https://kimkj38.github.io/images/2021-03-20/DCGAN(MNIST)_8_0.png)
    



```

```

## Reference
<https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html>
