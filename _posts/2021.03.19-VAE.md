---
title:  "VAE구현"

categories:
  - 코딩
tags:
  - VAE
  - 코딩
typora-copy-images-to: ..\images\2021-03-19
---

```
%matplotlib inline
import os
import torch
import torch.nn as nn
import torch.nn.functional as F

```

# Parameter Settings


```
latent_dims = 2
num_epochs = 100
batch_size = 128
capacity = 64
learning_rate = 1e-3
variational_beta = 1
use_gpu = True
```

# MNIST Data Loading


```
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST

img_transform = transforms.Compose([transforms.ToTensor()])

train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)
```

    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz



    HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))


    
    Extracting ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw
    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz



    HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))


    
    Extracting ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw
    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz



    HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))


    
    Extracting ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw
    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz



    HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))


    
    Extracting ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw
    Processing...
    Done!


    /usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)
      return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)


# VAE Definition


```
class Encoder(nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    c = capacity
    self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1)
    self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)
    self.fc_mu = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)
    self.fc_logvar = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)
  
  def forward(self, x):
    x = F.relu(self.conv1(x))
    x = F.relu(self.conv2(x))
    x = x.view(x.size(0), -1)
    x_mu = self.fc_mu(x)
    x_logvar = self.fc_logvar(x)
    return x_mu, x_logvar

class Decoder(nn.Module):
  def __init__(self):
    super(Decoder, self).__init__()
    c = capacity
    self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)
    self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)
    self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)

  def forward(self,x):
    x = self.fc(x)
    x = x.view(x.size(0), capacity*2, 7, 7)
    x = F.relu(self.conv2(x))
    x = torch.sigmoid(self.conv1(x))
    return x

class VariationalAutoencoder(nn.Module):
  def __init__(self):
    super(VariationalAutoencoder, self).__init__()
    self.encoder = Encoder()
    self.decoder = Decoder()

  def forward(self,x):
    latent_mu, latent_logvar = self.encoder(x)
    latent = self.latent_sample(latent_mu, latent_logvar)
    x_recon = self.decoder(latent)
    return x_recon, latent_mu, latent_logvar

  def latent_sample(self, mu, logvar):
    if self.training:
      std = logvar.mul(0.5).exp_()
      eps = torch.empty_like(std).normal_()
      return eps.mul(std).add_(mu)
    else:
      return mu

def vae_loss(recon_x, x, mu, logvar):
  recon_loss = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')
  kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
  return recon_loss + variational_beta * kldivergence

vae = VariationalAutoencoder()

num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)
print('Number of parameters: %d' % num_params)
```

    Number of parameters: 308357


# Train VAE


```
optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)

vae.train()
train_loss_avg = []

print('Training..')
for epoch in range(num_epochs):
  train_loss_avg.append(0)
  num_batches = 0
   
  for image_batch, _ in train_loader:
    
    # vae reconstruction
    image_batch_recon, latent_mu, latent_logvar = vae(image_batch)

    # reconstruction error
    loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)

    # backpropagation
    optimizer.zero_grad()
    loss.backward()

    # one step of the optmizer (using the gradients from backpropagation)
    optimizer.step()

    train_loss_avg[-1] += loss.item()
    num_batches += 1

  train_loss_avg[-1] /= num_batches
  print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))
```

    Training..
    Epoch [1 / 100] average reconstruction error: 23979.098572
    Epoch [2 / 100] average reconstruction error: 21445.405382
    Epoch [3 / 100] average reconstruction error: 20782.062360
    Epoch [4 / 100] average reconstruction error: 20378.040222
    Epoch [5 / 100] average reconstruction error: 20173.672512
    Epoch [6 / 100] average reconstruction error: 20024.246523
    Epoch [7 / 100] average reconstruction error: 19914.136027
    Epoch [8 / 100] average reconstruction error: 19829.906923
    Epoch [9 / 100] average reconstruction error: 19781.674047
    Epoch [10 / 100] average reconstruction error: 19705.084909
    Epoch [11 / 100] average reconstruction error: 19646.513884
    Epoch [12 / 100] average reconstruction error: 19607.874377
    Epoch [13 / 100] average reconstruction error: 19558.622149
    Epoch [14 / 100] average reconstruction error: 19518.101777
    Epoch [15 / 100] average reconstruction error: 19471.750704
    Epoch [16 / 100] average reconstruction error: 19430.597209
    Epoch [17 / 100] average reconstruction error: 19401.353232
    Epoch [18 / 100] average reconstruction error: 19362.540624
    Epoch [19 / 100] average reconstruction error: 19354.325100
    Epoch [20 / 100] average reconstruction error: 19317.631205
    Epoch [21 / 100] average reconstruction error: 19287.814576
    Epoch [22 / 100] average reconstruction error: 19252.230302
    Epoch [23 / 100] average reconstruction error: 19248.040653
    Epoch [24 / 100] average reconstruction error: 19203.526371
    Epoch [25 / 100] average reconstruction error: 19184.544422
    Epoch [26 / 100] average reconstruction error: 19163.015881
    Epoch [27 / 100] average reconstruction error: 19153.246083
    Epoch [28 / 100] average reconstruction error: 19122.253061
    Epoch [29 / 100] average reconstruction error: 19102.935362
    Epoch [30 / 100] average reconstruction error: 19088.773592
    Epoch [31 / 100] average reconstruction error: 19068.849160
    Epoch [32 / 100] average reconstruction error: 19050.019837
    Epoch [33 / 100] average reconstruction error: 19032.482899
    Epoch [34 / 100] average reconstruction error: 19015.552526
    Epoch [35 / 100] average reconstruction error: 19000.631051
    Epoch [36 / 100] average reconstruction error: 18984.578818
    Epoch [37 / 100] average reconstruction error: 18965.299736
    Epoch [38 / 100] average reconstruction error: 18958.522677
    Epoch [39 / 100] average reconstruction error: 18943.502663
    Epoch [40 / 100] average reconstruction error: 18940.265021
    Epoch [41 / 100] average reconstruction error: 18915.499473
    Epoch [42 / 100] average reconstruction error: 18900.991840
    Epoch [43 / 100] average reconstruction error: 18891.173149
    Epoch [44 / 100] average reconstruction error: 18874.598826
    Epoch [45 / 100] average reconstruction error: 18874.711631
    Epoch [46 / 100] average reconstruction error: 18868.484754
    Epoch [47 / 100] average reconstruction error: 18851.269331
    Epoch [48 / 100] average reconstruction error: 18832.010197
    Epoch [49 / 100] average reconstruction error: 18827.950514
    Epoch [50 / 100] average reconstruction error: 18812.837551
    Epoch [51 / 100] average reconstruction error: 18801.817656
    Epoch [52 / 100] average reconstruction error: 18788.739758
    Epoch [53 / 100] average reconstruction error: 18792.541913
    Epoch [54 / 100] average reconstruction error: 18773.318434
    Epoch [55 / 100] average reconstruction error: 18762.725122
    Epoch [56 / 100] average reconstruction error: 18763.651984
    Epoch [57 / 100] average reconstruction error: 18761.634703
    Epoch [58 / 100] average reconstruction error: 18749.615272
    Epoch [59 / 100] average reconstruction error: 18719.837966
    Epoch [60 / 100] average reconstruction error: 18734.188008
    Epoch [61 / 100] average reconstruction error: 18714.113896
    Epoch [62 / 100] average reconstruction error: 18706.573419
    Epoch [63 / 100] average reconstruction error: 18699.649824
    Epoch [64 / 100] average reconstruction error: 18694.023525
    Epoch [65 / 100] average reconstruction error: 18698.222523
    Epoch [66 / 100] average reconstruction error: 18680.876678
    Epoch [67 / 100] average reconstruction error: 18686.458224
    Epoch [68 / 100] average reconstruction error: 18670.256975
    Epoch [69 / 100] average reconstruction error: 18657.710136
    Epoch [70 / 100] average reconstruction error: 18650.980152
    Epoch [71 / 100] average reconstruction error: 18645.297685
    Epoch [72 / 100] average reconstruction error: 18640.113437
    Epoch [73 / 100] average reconstruction error: 18631.431031
    Epoch [74 / 100] average reconstruction error: 18630.425648
    Epoch [75 / 100] average reconstruction error: 18626.125768
    Epoch [76 / 100] average reconstruction error: 18609.099445
    Epoch [77 / 100] average reconstruction error: 18617.907424
    Epoch [78 / 100] average reconstruction error: 18614.707731
    Epoch [79 / 100] average reconstruction error: 18608.096365
    Epoch [80 / 100] average reconstruction error: 18611.286539
    Epoch [81 / 100] average reconstruction error: 18591.852583
    Epoch [82 / 100] average reconstruction error: 18590.764557
    Epoch [83 / 100] average reconstruction error: 18579.647163
    Epoch [84 / 100] average reconstruction error: 18571.916455
    Epoch [85 / 100] average reconstruction error: 18569.485693
    Epoch [86 / 100] average reconstruction error: 18567.493212
    Epoch [87 / 100] average reconstruction error: 18564.341943
    Epoch [88 / 100] average reconstruction error: 18559.092788
    Epoch [89 / 100] average reconstruction error: 18549.201513
    Epoch [90 / 100] average reconstruction error: 18544.437406
    Epoch [91 / 100] average reconstruction error: 18549.434704
    Epoch [92 / 100] average reconstruction error: 18534.185437
    Epoch [93 / 100] average reconstruction error: 18527.427091
    Epoch [94 / 100] average reconstruction error: 18533.969654
    Epoch [95 / 100] average reconstruction error: 18533.856089
    Epoch [96 / 100] average reconstruction error: 18524.789689
    Epoch [97 / 100] average reconstruction error: 18508.198561
    Epoch [98 / 100] average reconstruction error: 18510.479061
    Epoch [99 / 100] average reconstruction error: 18515.622832
    Epoch [100 / 100] average reconstruction error: 18502.497999


# Plot Training Curve


```
import matplotlib.pyplot as plt
plt.ion()

fig = plt.figure()
plt.plot(train_loss_avg)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()
```


    
![png]("kimkj38.github.io/images/2021-03-19/VAE_10_0.png")
    


# Evaluate on the Test Set


```
vae.eval()

test_loss_avg, num_batches = 0, 0
for image_batch, _ in test_loader:
  
  with torch.no_grad():
    
    # vae reconstruction
    image_batch_recon, latent_mu, latent_logvar = vae(image_batch)

    # reconstruction error
    loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)

    test_loss_avg += loss.item()
    num_batches += 1
  
test_loss_avg /= num_batches
print('average reconstruction error: %f' % (test_loss_avg))

```

    average reconstruction error: 19318.221235


# Visualize Reconstructions


```
import numpy as np
import matplotlib.pyplot as plt
plt.ion()

import torchvision.utils
vae.eval()

def to_img(x):
  x = x.clamp(0, 1)
  return x

def show_image(img):
  img = to_img(img)
  npimg = img.numpy()
  plt.imshow(np.transpose(npimg, (1,2,0)))

def visualze_output(images, model):

  with torch.no_grad():

    images, _, _ = model(images)
    images = to_img(images)
    np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()
    plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))
    plt.show()

images, labels = iter(test_loader).next()

# First visualise the original images
print('Original images')
show_image(torchvision.utils.make_grid(images[1:50], 10, 5))
plt.show()

# Reconstruct and visualise the images using the vae
print('VAE reconstructin')
visualze_output(images, vae)
```

    Original images



    
![png](VAE_files/VAE_14_1.png)
    


    VAE reconstructin



    
![png](VAE_files/VAE_14_3.png)
    


# Interpolate in Latent Space


```
vae.eval()

def interpolation(lambda1, model, img1, img2):

  with torch.no_grad():

    #latent vector of first image
    latent_1, _ = model.encoder(img1)

    #latent vector of second image
    latent_2, _ = model.encoder(img2)

    #interpolation of the two latent vectors
    inter_latent = lambda1 * latent_1 + (1-lambda1) * latent_2

    #reconstruction interpolated image
    inter_image = model.decoder(inter_latent)
    inter_image = inter_image.cpu()

    return inter_image

# sort part of test set by digit
digits = [[] for _ in range(10)]
for img_batch, label_batch in test_loader:
  for i in range(img_batch.size(0)):
    digits[label_batch[i]].append(img_batch[i:i+1])
  if sum(len(d) for d in digits) >= 1000:
    break;

# interpolation lambdas
lambda_range = np.linspace(0, 1, 10)

fig, ax = plt.subplots(2, 5, figsize=(15,6))
fig.subplots_adjust(hspace= .5, wspace=.001)
ax = ax.ravel()

for ind, l in enumerate(lambda_range):
  inter_image = interpolation(float(l), vae, digits[7][0], digits[1][0])

  inter_image = to_img(inter_image)

  image = inter_image.numpy()

  ax[ind].imshow(image[0,0,:,:], cmap='gray')
  ax[ind].set_title('lambda_val='+str(round(l,1)))
plt.show()
```


    
![png](VAE_files/VAE_16_0.png)
    


# Sample Latent Vector from Prior (VAE as Generator)


```
vae.eval()

with torch.no_grad():
  # sample latent vectors from the normal distribution
  latent = torch.randn(128, latent_dims)

  # reconstruct images from the latent vectors
  img_recon = vae.decoder(latent)
  img_recon = img_recon.cpu()

  fig, ax = plt.subplots(figsize=(5,5))
  show_image(torchvision.utils.make_grid(img_recon.data[:100],10,5))
  plt.show()
```


    
![png](VAE_files/VAE_18_0.png)
    


# Show 2D Latent Space


```
# load a network that was trained with a 2d latent space
if latent_dims != 2:
    print('Please change the parameters to two latent dimensions.')
    
with torch.no_grad():
    
    # create a sample grid in 2d latent space
    latent_x = np.linspace(-1.5,1.5,20)
    latent_y = np.linspace(-1.5,1.5,20)
    latents = torch.FloatTensor(len(latent_y), len(latent_x), 2)
    for i, lx in enumerate(latent_x):
        for j, ly in enumerate(latent_y):
            latents[j, i, 0] = lx
            latents[j, i, 1] = ly
    latents = latents.view(-1, 2) # flatten grid into a batch

    # reconstruct images from the latent vectors
    image_recon = vae.decoder(latents)
    image_recon = image_recon.cpu()

    fig, ax = plt.subplots(figsize=(10, 10))
    show_image(torchvision.utils.make_grid(image_recon.data[:400],20,5))
    plt.show()
```


    
![png](VAE_files/VAE_20_0.png)
    



```

```
